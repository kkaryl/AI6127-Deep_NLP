{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXxqwSh0C0Ou"
   },
   "source": [
    "# Using Embeddings for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPAKy-ytQmgM"
   },
   "source": [
    "## Colab configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4441,
     "status": "ok",
     "timestamp": 1586004182079,
     "user": {
      "displayName": "Banausic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYVsjWa_XDZoiSoyArYulTOek6XUfsvUZixt6_=s64",
      "userId": "07824873713850476084"
     },
     "user_tz": -480
    },
    "id": "NE9SwK_FC2Dl",
    "outputId": "a17813f1-3ba6-477e-ed52-271b497095f4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "colab = False # Change to True if using Colab\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    gdrivedir = '/content/drive/My Drive/Colab Notebooks/deepnlpa3/'\n",
    "    os.chdir(gdrivedir)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMncl9roXEwq"
   },
   "source": [
    "### Download dataset (Skip if not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10446,
     "status": "ok",
     "timestamp": 1586004188102,
     "user": {
      "displayName": "Banausic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYVsjWa_XDZoiSoyArYulTOek6XUfsvUZixt6_=s64",
      "userId": "07824873713850476084"
     },
     "user_tz": -480
    },
    "id": "bTaOBsYcUxAZ",
    "outputId": "ea5ffa25-1248-40f9-9fe0-2538903c6e4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/karyl/Virtual/Python/deepnlpa3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/karyl/Virtual/Python/deepnlpa3/lib/python3.6/site-packages (from requests)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/karyl/Virtual/Python/deepnlpa3/lib/python3.6/site-packages (from requests)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/karyl/Virtual/Python/deepnlpa3/lib/python3.6/site-packages (from requests)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/karyl/Virtual/Python/deepnlpa3/lib/python3.6/site-packages (from requests)\n",
      "mkdir: cannot create directory ‘data/’: File exists\n",
      "mkdir: cannot create directory ‘data/ag_news/’: File exists\n",
      "/home/karyl/Virtual/Python/deepnlpa3/src\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "import requests\n",
    "\n",
    "def progress_bar(some_iter):\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        return tqdm(some_iter)\n",
    "    except ModuleNotFoundError:\n",
    "        return some_iter\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    print(\"Trying to fetch {}\".format(destination))\n",
    "\n",
    "    def get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "\n",
    "        return None\n",
    "\n",
    "    def save_response_content(response, destination):\n",
    "        CHUNK_SIZE = 32768\n",
    "\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in progress_bar(response.iter_content(CHUNK_SIZE)):\n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)\n",
    "\n",
    "if not colab:\n",
    "    gdrivedir = ''\n",
    "    \n",
    "datadir = 'data/ag_news/'\n",
    "!mkdir 'data/'\n",
    "!mkdir 'data/ag_news/'\n",
    "\n",
    "filename = gdrivedir + datadir + 'news_with_splits.csv'\n",
    "if not os.path.exists(filename):\n",
    "    download_file_from_google_drive('1Z4fOgvrNhcn6pYlOxrEuxrPNxT-bLh7T', filename)\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zirxAlI0QqBU"
   },
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13696,
     "status": "ok",
     "timestamp": 1586004191367,
     "user": {
      "displayName": "Banausic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYVsjWa_XDZoiSoyArYulTOek6XUfsvUZixt6_=s64",
      "userId": "07824873713850476084"
     },
     "user_tz": -480
    },
    "id": "B0Ph_BZLC_ip",
    "outputId": "f01edd45-5332-40b2-eb55-86c49a5f0396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/karyl/Virtual/Python/deepnlpa3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: pandas in /home/karyl/Virtual/Python/deepnlpa3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/karyl/Virtual/Python/deepnlpa3/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/karyl/Virtual/Python/deepnlpa3/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/karyl/Virtual/Python/deepnlpa3/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /home/karyl/Virtual/Python/deepnlpa3/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas)\n",
      "Requirement already satisfied: tqdm in /home/karyl/Virtual/Python/deepnlpa3/lib/python3.6/site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install pandas\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_jP01aDGC0Ov"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6vHmQqo3C0Ow"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter, defaultdict # defaultdict NEW\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm #from tqdm import tqdm_notebook\n",
    "\n",
    "### NEW ####\n",
    "import sentencepiece as spm\n",
    "import tempfile\n",
    "import sys\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZZ79vcyC0O1"
   },
   "source": [
    "## Data Vectorization classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqbtzx1LC0O2"
   },
   "source": [
    "### The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwszpKHMC0O3"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icULWyCqC0O6"
   },
   "outputs": [],
   "source": [
    "class WordSequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(WordSequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(WordSequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IaAAOmU-C0O-"
   },
   "outputs": [],
   "source": [
    "class CharacterSequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, pad_token=\"<PAD>\"):\n",
    "\n",
    "        super(CharacterSequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._pad_token = pad_token\n",
    "\n",
    "        self.pad_index = self.add_token(self._pad_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(CharacterSequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'pad_token': self._pad_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tR-Qqsm4BAp7"
   },
   "outputs": [],
   "source": [
    "class SubwordSequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, \n",
    "                 pad_token=\"<PAD>\", unk_token=\"<UNK>\",\n",
    "                 begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "        \n",
    "        super(SubwordSequenceVocabulary, self).__init__(token_to_idx)\n",
    "        self.token_freq = {}\n",
    "        self.bpe_codes = []\n",
    "        \n",
    "        self._pad_token = pad_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.pad_index = self.add_token(self._pad_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        contents = super(SubwordSequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'token_freq': self.token_freq})\n",
    "        contents.update({'bpe_codes': self.bpe_codes})\n",
    "        contents.update({'pad_token': self._pad_token, \n",
    "                         'unk_token': self._unk_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "    \n",
    "    def add_bpe_codes_list(self, bpe_code_list):\n",
    "        self.bpe_codes.extend(bpe_code_list)\n",
    "    \n",
    "    def add_bpe_token(self, token, frequency):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "            \n",
    "        self.token_freq[token] = frequency\n",
    "        return index\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgVS8SoADIGS"
   },
   "outputs": [],
   "source": [
    "class SentenceSequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, pad_token=\"<pad>\", bos_token=\"<s>\",\n",
    "                 eos_token=\"</s>\", unk_token=\"<unk>\"):\n",
    "\n",
    "        super(SentenceSequenceVocabulary, self).__init__(token_to_idx)\n",
    "        self._pad_token = pad_token\n",
    "        self._bos_token = bos_token\n",
    "        self._eos_token = eos_token\n",
    "        self._unk_token = unk_token\n",
    "\n",
    "        self.pad_index = self.add_token(self._pad_token) # 0\n",
    "        self.bos_index = self.add_token(self._bos_token) # 1\n",
    "        self.eos_index = self.add_token(self._eos_token) # 2\n",
    "        self.unk_index = self.add_token(self._unk_token) # 3\n",
    "\n",
    "        self.sp_segmenter = None\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SentenceSequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'pad_token': self._pad_token , 'bos_token': self._bos_token,\n",
    "                         'eos_token' : self._eos_token, 'unk_token': self._unk_token })\n",
    "        return contents\n",
    "\n",
    "    def load_vocab_file(self, vocab_file):\n",
    "        with open(vocab_file, encoding='utf-8') as f:\n",
    "            vo = [doc.strip().split(\"\\t\") for doc in f]\n",
    "\n",
    "        for i, w in enumerate(vo):  # w[0]: token name, w[1]: token score\n",
    "            self.add_token(w[0])    # add_token will skip duplicates\n",
    "\n",
    "    def load_model_file(self, model_file):\n",
    "        self.sp_segmenter = spm.SentencePieceProcessor()\n",
    "        self.sp_segmenter.load(model_file)\n",
    "        self.sp_segmenter.SetEncodeExtraOptions('bos:eos') # auto append bos eos tokens\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNSjU3bOEJ2d"
   },
   "source": [
    "### BPE Trainer and Segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InjM2OFAEIrW"
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(fobj, is_dict=False):\n",
    "    \"\"\"Read text and return dictionary that encodes vocabulary\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "    for i, line in enumerate(fobj):\n",
    "        if is_dict:\n",
    "            try:\n",
    "                word, count = line.strip('\\r\\n ').split(' ')\n",
    "            except:\n",
    "                print('Failed reading vocabulary file at line {0}: {1}'.format(i, line))\n",
    "                sys.exit(1)\n",
    "            vocab[word] += int(count)\n",
    "        else:\n",
    "            for word in line.strip('\\r\\n ').split(' '):\n",
    "                if word:\n",
    "                    vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "def update_pair_statistics(pair, changed, stats, indices):\n",
    "    \"\"\"Minimally update the indices and frequency of symbol pairs\n",
    "    if we merge a pair of symbols, only pairs that overlap with occurrences\n",
    "    of this pair are affected, and need to be updated.\n",
    "    \"\"\"\n",
    "    stats[pair] = 0\n",
    "    indices[pair] = defaultdict(int)\n",
    "    first, second = pair\n",
    "    new_pair = first+second\n",
    "    for j, word, old_word, freq in changed:\n",
    "\n",
    "        # find all instances of pair, and update frequency/indices around it\n",
    "        i = 0\n",
    "        while True:\n",
    "            # find first symbol\n",
    "            try:\n",
    "                i = old_word.index(first, i)\n",
    "            except ValueError:\n",
    "                break\n",
    "            # if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])\n",
    "            if i < len(old_word)-1 and old_word[i+1] == second:\n",
    "                # assuming a symbol sequence \"A B C\", if \"B C\" is merged, reduce the frequency of \"A B\"\n",
    "                if i:\n",
    "                    prev = old_word[i-1:i+1]\n",
    "                    stats[prev] -= freq\n",
    "                    indices[prev][j] -= 1\n",
    "                if i < len(old_word)-2:\n",
    "                    # assuming a symbol sequence \"A B C B\", if \"B C\" is merged, reduce the frequency of \"C B\".\n",
    "                    # however, skip this if the sequence is A B C B C, because the frequency of \"C B\" will be reduced by the previous code block\n",
    "                    if old_word[i+2] != first or i >= len(old_word)-3 or old_word[i+3] != second:\n",
    "                        nex = old_word[i+1:i+3]\n",
    "                        stats[nex] -= freq\n",
    "                        indices[nex][j] -= 1\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        i = 0\n",
    "        while True:\n",
    "            try:\n",
    "                # find new pair\n",
    "                i = word.index(new_pair, i)\n",
    "            except ValueError:\n",
    "                break\n",
    "            # assuming a symbol sequence \"A BC D\", if \"B C\" is merged, increase the frequency of \"A BC\"\n",
    "            if i:\n",
    "                prev = word[i-1:i+1]\n",
    "                stats[prev] += freq\n",
    "                indices[prev][j] += 1\n",
    "            # assuming a symbol sequence \"A BC B\", if \"B C\" is merged, increase the frequency of \"BC B\"\n",
    "            # however, if the sequence is A BC BC, skip this step because the count of \"BC BC\" will be incremented by the previous code block\n",
    "            if i < len(word)-1 and word[i+1] != new_pair:\n",
    "                nex = word[i:i+2]\n",
    "                stats[nex] += freq\n",
    "                indices[nex][j] += 1\n",
    "            i += 1\n",
    "\n",
    "\n",
    "def get_pair_statistics(vocab):\n",
    "    \"\"\"Count frequency of all symbol pairs, and create index\"\"\"\n",
    "\n",
    "    # data structure of pair frequencies\n",
    "    stats = defaultdict(int)\n",
    "\n",
    "    #index from pairs to words\n",
    "    indices = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for i, (word, freq) in enumerate(vocab):\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            stats[prev_char, char] += freq\n",
    "            indices[prev_char, char][i] += 1\n",
    "            prev_char = char\n",
    "\n",
    "    return stats, indices\n",
    "\n",
    "\n",
    "def replace_pair(pair, vocab, indices):\n",
    "    \"\"\"Replace all occurrences of a symbol pair ('A', 'B') with a new symbol 'AB'\"\"\"\n",
    "    first, second = pair\n",
    "    pair_str = ''.join(pair)\n",
    "    pair_str = pair_str.replace('\\\\','\\\\\\\\')\n",
    "    changes = []\n",
    "    pattern = re.compile(r'(?<!\\S)' + re.escape(first + ' ' + second) + r'(?!\\S)')\n",
    "    if sys.version_info < (3, 0):\n",
    "        iterator = indices[pair].iteritems()\n",
    "    else:\n",
    "        iterator = indices[pair].items()\n",
    "    for j, freq in iterator:\n",
    "        if freq < 1:\n",
    "            continue\n",
    "        word, freq = vocab[j]\n",
    "        new_word = ' '.join(word)\n",
    "        new_word = pattern.sub(pair_str, new_word)\n",
    "        new_word = tuple(new_word.split(' '))\n",
    "\n",
    "        vocab[j] = (new_word, freq)\n",
    "        changes.append((j, new_word, word, freq))\n",
    "\n",
    "    return changes\n",
    "\n",
    "def prune_stats(stats, big_stats, threshold):\n",
    "    \"\"\"Prune statistics dict for efficiency of max()\n",
    "    The frequency of a symbol pair never increases, so pruning is generally safe\n",
    "    (until we the most frequent pair is less frequent than a pair we previously pruned)\n",
    "    big_stats keeps full statistics for when we need to access pruned items\n",
    "    \"\"\"\n",
    "    for item,freq in list(stats.items()):\n",
    "        if freq < threshold:\n",
    "            del stats[item]\n",
    "            if freq < 0:\n",
    "                big_stats[item] += freq\n",
    "            else:\n",
    "                big_stats[item] = freq\n",
    "\n",
    "\n",
    "def learn_bpe(data, num_symbols, min_frequency=2, verbose=False, is_dict=False, total_symbols=False):\n",
    "    \"\"\"Learn num_symbols BPE operations from vocabulary, and write to list of tuples.\n",
    "    \"\"\"\n",
    "    vocab = get_vocabulary(data, is_dict) # count words in text\n",
    "\n",
    "    vocab = dict([(tuple(x[:-1])+(x[-1]+'</w>',) ,y) for (x,y) in vocab.items()])\n",
    "    \n",
    "    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    stats, indices = get_pair_statistics(sorted_vocab)\n",
    "\n",
    "    big_stats = copy.deepcopy(stats)\n",
    "\n",
    "    if total_symbols:\n",
    "        uniq_char_internal = set()\n",
    "        uniq_char_final = set()\n",
    "        for word in vocab:\n",
    "            for char in word[:-1]:\n",
    "                uniq_char_internal.add(char)\n",
    "            uniq_char_final.add(word[-1])\n",
    "        sys.stderr.write('Number of word-internal characters: {0}\\n'.format(len(uniq_char_internal)))\n",
    "        sys.stderr.write('Number of word-final characters: {0}\\n'.format(len(uniq_char_final)))\n",
    "        sys.stderr.write('Reducing number of merge operations by {0}\\n'.format(len(uniq_char_internal) + len(uniq_char_final)))\n",
    "        num_symbols -= len(uniq_char_internal) + len(uniq_char_final)\n",
    "        sys.stderr.write('Number of symbols left: {0}\\n'.format(num_symbols))\n",
    "\n",
    "    bpe_codes = []\n",
    "    # threshold is inspired by Zipfian assumption, but should only affect speed\n",
    "    threshold = max(stats.values()) / 10\n",
    "    for i in range(num_symbols):\n",
    "        if stats:\n",
    "            most_frequent = max(stats, key=lambda x: (stats[x], x))\n",
    "\n",
    "        # we probably missed the best pair because of pruning; go back to full statistics\n",
    "        if not stats or (i and stats[most_frequent] < threshold):\n",
    "            prune_stats(stats, big_stats, threshold)\n",
    "            stats = copy.deepcopy(big_stats)\n",
    "            most_frequent = max(stats, key=lambda x: (stats[x], x))\n",
    "            # threshold is inspired by Zipfian assumption, but should only affect speed\n",
    "            threshold = stats[most_frequent] * i/(i+10000.0)\n",
    "            prune_stats(stats, big_stats, threshold)\n",
    "\n",
    "        if stats[most_frequent] < min_frequency:\n",
    "            sys.stderr.write('no pair has frequency >= {0}. Stopping\\n'.format(min_frequency))\n",
    "            break\n",
    "\n",
    "        if verbose:\n",
    "            sys.stderr.write('pair {0}: {1} {2} -> {1}{2} (frequency {3})\\n'.format(i, most_frequent[0], most_frequent[1], stats[most_frequent]))\n",
    "\n",
    "        bpe_codes.append((most_frequent[0], most_frequent[1]))\n",
    "        changes = replace_pair(most_frequent, sorted_vocab, indices)\n",
    "        update_pair_statistics(most_frequent, changes, stats, indices)\n",
    "        stats[most_frequent] = 0\n",
    "        if not i % 100:\n",
    "            prune_stats(stats, big_stats, threshold)\n",
    "            \n",
    "    if verbose:\n",
    "        print(bpe_codes)\n",
    "    \n",
    "    return bpe_codes\n",
    "\n",
    "class BPE(object):\n",
    "\n",
    "    def __init__(self, codes, merges=-1, separator='@@', vocab=None, glossaries=None):\n",
    "        self.version = (0, 2) #Hardcode\n",
    "        self.bpe_codes = codes\n",
    "        \n",
    "        # some hacking to deal with duplicates (only consider first instance)\n",
    "        self.bpe_codes = dict([(code,i) for (i,code) in reversed(list(enumerate(self.bpe_codes)))])\n",
    "        \n",
    "        self.bpe_codes_reverse = dict([(pair[0] + pair[1], pair) for pair,i in self.bpe_codes.items()])\n",
    "\n",
    "        self.separator = separator\n",
    "\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.glossaries = glossaries if glossaries else []\n",
    "\n",
    "        self.glossaries_regex = re.compile('^({})$'.format('|'.join(glossaries))) if glossaries else None\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def process_line(self, line, dropout=0):\n",
    "        \"\"\"segment line, dealing with leading and trailing whitespace\"\"\"\n",
    "\n",
    "        out = \"\"\n",
    "\n",
    "        leading_whitespace = len(line)-len(line.lstrip('\\r\\n '))\n",
    "        if leading_whitespace:\n",
    "            out += line[:leading_whitespace]\n",
    "\n",
    "        out += self.segment(line, dropout)\n",
    "\n",
    "        trailing_whitespace = len(line)-len(line.rstrip('\\r\\n '))\n",
    "        if trailing_whitespace and trailing_whitespace != len(line):\n",
    "            out += line[-trailing_whitespace:]\n",
    "        \n",
    "        # print(out)\n",
    "        return out\n",
    "\n",
    "    def segment(self, sentence, dropout=0):\n",
    "        \"\"\"segment single sentence (whitespace-tokenized string) with BPE encoding\"\"\"\n",
    "        segments = self.segment_tokens(sentence.strip('\\r\\n ').split(' '), dropout)\n",
    "        return ' '.join(segments)\n",
    "\n",
    "    def segment_tokens(self, tokens, dropout=0):\n",
    "        \"\"\"segment a sequence of tokens with BPE encoding\"\"\"\n",
    "        output = []\n",
    "        for word in tokens:\n",
    "            # eliminate double spaces\n",
    "            if not word:\n",
    "                continue\n",
    "            new_word = [out for segment in self._isolate_glossaries(word)\n",
    "                        for out in encode(segment,\n",
    "                                          self.bpe_codes,\n",
    "                                          self.bpe_codes_reverse,\n",
    "                                          self.vocab,\n",
    "                                          self.separator,\n",
    "                                          self.version,\n",
    "                                          self.cache,\n",
    "                                          self.glossaries_regex,\n",
    "                                          dropout)]\n",
    "\n",
    "            for item in new_word[:-1]:\n",
    "                output.append(item + self.separator)\n",
    "            output.append(new_word[-1])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _isolate_glossaries(self, word):\n",
    "        word_segments = [word]\n",
    "        for gloss in self.glossaries:\n",
    "            word_segments = [out_segments for segment in word_segments\n",
    "                                 for out_segments in isolate_glossary(segment, gloss)]\n",
    "        return word_segments\n",
    "\n",
    "def encode(orig, bpe_codes, bpe_codes_reverse, vocab, separator, version, cache, glossaries_regex=None, dropout=0):\n",
    "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\n",
    "    \"\"\"\n",
    "\n",
    "    if not dropout and orig in cache:\n",
    "        return cache[orig]\n",
    "\n",
    "    if glossaries_regex and glossaries_regex.match(orig):\n",
    "        cache[orig] = (orig,)\n",
    "        return (orig,)\n",
    "\n",
    "    if len(orig) == 1:\n",
    "        return orig\n",
    "\n",
    "    if version == (0, 1):\n",
    "        word = list(orig) + ['</w>']\n",
    "    elif version == (0, 2): # more consistent handling of word-final segments\n",
    "        word = list(orig[:-1]) + [orig[-1] + '</w>']\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    while len(word) > 1:\n",
    "\n",
    "        # get list of symbol pairs; optionally apply dropout\n",
    "        pairs = [(bpe_codes[pair],i,pair) for (i,pair) in enumerate(zip(word, word[1:])) if (not dropout or random.random() > dropout) and pair in bpe_codes]\n",
    "\n",
    "        if not pairs:\n",
    "            break\n",
    "\n",
    "        #get first merge operation in list of BPE codes\n",
    "        bigram = min(pairs)[2]\n",
    "\n",
    "        # find start position of all pairs that we want to merge\n",
    "        positions = [i for (rank,i,pair) in pairs if pair == bigram]\n",
    "\n",
    "        i = 0\n",
    "        new_word = []\n",
    "        bigram = ''.join(bigram)\n",
    "        for j in positions:\n",
    "            # merges are invalid if they start before current position. This can happen if there are overlapping pairs: (x x x -> xx x)\n",
    "            if j < i:\n",
    "                continue\n",
    "            new_word.extend(word[i:j]) # all symbols before merged pair\n",
    "            new_word.append(bigram) # merged pair\n",
    "            i = j+2 # continue after merged pair\n",
    "        new_word.extend(word[i:]) # add all symbols until end of word\n",
    "        word = new_word\n",
    "\n",
    "    # don't print end-of-word symbols\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word[-1] = word[-1][:-4]\n",
    "\n",
    "    word = tuple(word)\n",
    "    if vocab:\n",
    "        word = check_vocab_and_split(word, bpe_codes_reverse, vocab, separator)\n",
    "\n",
    "    cache[orig] = word\n",
    "    return word\n",
    "\n",
    "def recursive_split(segment, bpe_codes, vocab, separator, final=False):\n",
    "    \"\"\"Recursively split segment into smaller units (by reversing BPE merges)\n",
    "    until all units are either in-vocabulary, or cannot be split futher.\"\"\"\n",
    "\n",
    "    try:\n",
    "        if final:\n",
    "            left, right = bpe_codes[segment + '</w>']\n",
    "            right = right[:-4]\n",
    "        else:\n",
    "            left, right = bpe_codes[segment]\n",
    "    except:\n",
    "        #sys.stderr.write('cannot split {0} further.\\n'.format(segment))\n",
    "        yield segment\n",
    "        return\n",
    "\n",
    "    if left + separator in vocab:\n",
    "        yield left\n",
    "    else:\n",
    "        for item in recursive_split(left, bpe_codes, vocab, separator, False):\n",
    "            yield item\n",
    "\n",
    "    if (final and right in vocab) or (not final and right + separator in vocab):\n",
    "        yield right\n",
    "    else:\n",
    "        for item in recursive_split(right, bpe_codes, vocab, separator, final):\n",
    "            yield item\n",
    "\n",
    "def check_vocab_and_split(orig, bpe_codes, vocab, separator):\n",
    "    \"\"\"Check for each segment in word if it is in-vocabulary,\n",
    "    and segment OOV segments into smaller units by reversing the BPE merge operations\"\"\"\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for segment in orig[:-1]:\n",
    "        if segment + separator in vocab:\n",
    "            out.append(segment)\n",
    "        else:\n",
    "            #sys.stderr.write('OOV: {0}\\n'.format(segment))\n",
    "            for item in recursive_split(segment, bpe_codes, vocab, separator, False):\n",
    "                out.append(item)\n",
    "\n",
    "    segment = orig[-1]\n",
    "    if segment in vocab:\n",
    "        out.append(segment)\n",
    "    else:\n",
    "        #sys.stderr.write('OOV: {0}\\n'.format(segment))\n",
    "        for item in recursive_split(segment, bpe_codes, vocab, separator, True):\n",
    "            out.append(item)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def read_vocabulary(vocab_file, threshold):\n",
    "    \"\"\"read vocabulary file produced by get_vocab.py, and filter according to frequency threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    vocabulary = set()\n",
    "\n",
    "    for line in vocab_file:\n",
    "        word, freq = line.strip('\\r\\n ').split(' ')\n",
    "        freq = int(freq)\n",
    "        if threshold == None or freq >= threshold:\n",
    "            vocabulary.add(word)\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "def isolate_glossary(word, glossary):\n",
    "    \"\"\"\n",
    "    Isolate a glossary present inside a word.\n",
    "\n",
    "    Returns a list of subwords. In which all 'glossary' glossaries are isolated \n",
    "\n",
    "    For example, if 'USA' is the glossary and '1934USABUSA' the word, the return value is:\n",
    "        ['1934', 'USA', 'B', 'USA']\n",
    "    \"\"\"\n",
    "    # regex equivalent of (if word == glossary or glossary not in word)\n",
    "    if re.match('^'+glossary+'$', word) or not re.search(glossary, word):\n",
    "        return [word]\n",
    "    else:\n",
    "        segments = re.split(r'({})'.format(glossary), word)\n",
    "        segments, ending = segments[:-1], segments[-1]\n",
    "        segments = list(filter(None, segments)) # Remove empty strings in regex group.\n",
    "        return segments + [ending.strip('\\r\\n ')] if ending != '' else segments\n",
    "    \n",
    "### CALL THIS FUNCTION TO TRAIN AND CREATE VOCAB ###             \n",
    "def learn_joint_bpe_and_vocab(data, symbols, min_frequency, total_symbols, dropout=0, separator=\"@@\", verbose=False):\n",
    "    # get combined vocabulary of all input texts\n",
    "    full_vocab = get_vocabulary(data)\n",
    "    \n",
    "    vocab_list = ['{0} {1}'.format(key, freq) for (key, freq) in full_vocab.items()]\n",
    "\n",
    "    # learn BPE on combined vocabulary\n",
    "    bpe_codes = learn_bpe(vocab_list, symbols, min_frequency, verbose, is_dict=True, total_symbols=total_symbols)\n",
    "\n",
    "    bpe = BPE(bpe_codes, separator=separator)\n",
    "    \n",
    "    # apply BPE to each training corpus and get vocabulary\n",
    "    segments = []\n",
    "    \n",
    "    for line in data:\n",
    "        line_segment = bpe.segment(line, dropout=dropout).strip()\n",
    "        segments.append(line_segment)\n",
    "              \n",
    "    vocab = get_vocabulary(segments)\n",
    "    \n",
    "    subword_vocab = SubwordSequenceVocabulary()\n",
    "    subword_vocab.add_bpe_codes_list(bpe_codes)\n",
    "    \n",
    "    for key, freq in sorted(vocab.items(), key=lambda x: x[1], reverse=True):\n",
    "        subword_vocab.add_bpe_token(key, freq)\n",
    "\n",
    "    return subword_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyTm3ARbC0PB"
   },
   "source": [
    "### The Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ONBp_6MC0PC"
   },
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
    "    def __init__(self, title_vocab, category_vocab, mode): # , title_vocab\n",
    "        self.mode = mode\n",
    "        self.title_vocab = title_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "        if \"bpe\" in mode:\n",
    "            self.bpe = BPE(self.title_vocab.bpe_codes, vocab = self.title_vocab.token_freq)\n",
    "                \n",
    "    def vectorize(self, title, max_seq_length, max_word_length, max_sent_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word (str): a word\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            the vetorized title (numpy.array)\n",
    "        \"\"\"\n",
    "        if self.mode == \"word\":\n",
    "            indices = [self.title_vocab.begin_seq_index]\n",
    "            indices.extend(self.title_vocab.lookup_token(token) \n",
    "                          for token in title.split(\" \"))\n",
    "            indices.append(self.title_vocab.end_seq_index)\n",
    "\n",
    "            vector_length = max_seq_length\n",
    "            if vector_length < 0: \n",
    "                vector_length = len(indices)\n",
    "\n",
    "            out_vectors = np.zeros(vector_length, dtype=np.int64)\n",
    "            out_vectors[:len(indices)] = indices\n",
    "            out_vectors[len(indices):] = self.title_vocab.mask_index\n",
    "\n",
    "        elif self.mode == \"char\":\n",
    "            words = title.split(\" \")\n",
    "            if len(words) > max_seq_length:\n",
    "                words = words[:max_seq_length]\n",
    "                \n",
    "            out_vectors = []\n",
    "            for word in words:\n",
    "                word_indices = [self.title_vocab.lookup_token(token) for token in word]\n",
    "                if len(word_indices) > max_word_length:\n",
    "                    word_indices = word_indices[:max_word_length]\n",
    "\n",
    "                out_vector = np.zeros(max_word_length, dtype=np.int64)\n",
    "                out_vector[:len(word_indices)] = word_indices\n",
    "                if len(word_indices) < max_word_length:\n",
    "                    out_vector[len(word_indices):] = self.title_vocab.pad_index\n",
    "                \n",
    "                out_vectors.append(out_vector)\n",
    "                \n",
    "            if len(words) < max_seq_length:\n",
    "                null_word_emb = np.array([self.title_vocab.pad_index] * max_word_length, dtype=np.int64)\n",
    "                for _ in range(max_seq_length - len(words)):\n",
    "                    out_vectors.append(null_word_emb)\n",
    "\n",
    "            out_vectors = np.array(out_vectors, dtype=np.int64)\n",
    "\n",
    "        elif self.mode == \"bpe-char\":\n",
    "            words = title.strip().split() # segment title into words\n",
    "            out_vectors = []  \n",
    "            for titleword in words:\n",
    "                encoded = self.bpe.process_line(titleword.strip())\n",
    "                subwords = encoded.strip().split()\n",
    "                word_indices = [self.title_vocab.lookup_token(token) for token in subwords]\n",
    "                if len(word_indices) > max_word_length:\n",
    "                    word_indices = word_indices[:max_word_length]\n",
    "                    \n",
    "                out_vector = np.zeros(max_word_length, dtype=np.int64)\n",
    "                out_vector[:len(word_indices)] = word_indices\n",
    "                if len(word_indices) < max_word_length:\n",
    "                    out_vector[len(word_indices):] = self.title_vocab.pad_index\n",
    "                \n",
    "                out_vectors.append(out_vector) # append each subword as a rep of each word\n",
    "\n",
    "            if len(words) < max_seq_length:\n",
    "                null_word_emb = np.array([self.title_vocab.pad_index] * max_word_length, dtype=np.int64)\n",
    "                for _ in range(max_seq_length - len(words)):\n",
    "                    out_vectors.append(null_word_emb)\n",
    "\n",
    "            out_vectors = np.array(out_vectors, dtype=np.int64)\n",
    "\n",
    "        elif self.mode == \"bpe-word\":\n",
    "            indices = [self.title_vocab.begin_seq_index]\n",
    "            encoded = self.bpe.process_line(title.strip())\n",
    "            indices.extend(self.title_vocab.lookup_token(token) \n",
    "                          for token in encoded.strip().split())\n",
    "            indices.append(self.title_vocab.end_seq_index)\n",
    "\n",
    "            vector_length = max_sent_length\n",
    "            out_vectors = np.zeros(vector_length, dtype=np.int64)\n",
    "            out_vectors[:len(indices)] = indices\n",
    "            out_vectors[len(indices):] = self.title_vocab.pad_index\n",
    "\n",
    "        elif self.mode == \"sent\":\n",
    "            # words = self.title_vocab.sp_segmenter.encode_as_pieces(title) # for debugging\n",
    "            # spm already configured to auto add bos and eos to ids\n",
    "            # encodes entire sentence into tokens\n",
    "            indices = self.title_vocab.sp_segmenter.encode_as_ids(title) \n",
    "\n",
    "            vector_length = max_sent_length\n",
    "            out_vectors = np.zeros(vector_length, dtype=np.int64)\n",
    "            out_vectors[:len(indices)] = indices\n",
    "            out_vectors[len(indices):] = self.title_vocab.pad_index\n",
    "          \n",
    "        return out_vectors\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, news_df, mode, vocab_size, cutoff=25, delete_files=False):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            news_df (pandas.DataFrame): the target dataset\n",
    "            cutoff (int): frequency threshold for including in Vocabulary \n",
    "        Returns:\n",
    "            an instance of the NewsVectorizer\n",
    "        \"\"\"\n",
    "        category_vocab = Vocabulary()        \n",
    "        for category in sorted(set(news_df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "\n",
    "        if mode == \"word\":\n",
    "            word_counts = Counter()\n",
    "            for title in news_df.title:\n",
    "                for token in title.split(\" \"):\n",
    "                    if token not in string.punctuation:\n",
    "                        word_counts[token] += 1\n",
    "            \n",
    "            title_vocab = WordSequenceVocabulary()\n",
    "            for word, word_count in word_counts.items():\n",
    "                if word_count >= cutoff:        \n",
    "                    title_vocab.add_token(word)\n",
    "\n",
    "        elif mode == \"char\":\n",
    "            title_vocab = CharacterSequenceVocabulary()\n",
    "            for title in news_df.title:\n",
    "                for token in title.split(\" \"):\n",
    "                    title_vocab.add_many(list(token))\n",
    "\n",
    "        elif \"bpe\" in mode:\n",
    "            title_vocab = SubwordSequenceVocabulary()\n",
    "            total_symbols = False\n",
    "            separator = \"@@\"\n",
    "            min_frequency = 0\n",
    "            title_vocab = learn_joint_bpe_and_vocab(news_df.title, vocab_size, \n",
    "                                                    min_frequency, total_symbols, \n",
    "                                                    dropout=0, separator=separator, verbose=False)\n",
    "            for title in news_df.title:\n",
    "                for token in title.split(\" \"):\n",
    "                    title_vocab.add_many(list(token))\n",
    "            \n",
    "#             title_vocab.compress_dict(vocab_size)\n",
    "\n",
    "        elif mode == \"sent\":\n",
    "            title_vocab = SentenceSequenceVocabulary()\n",
    "            news_df_title = news_df.title\n",
    "\n",
    "            # create a temporary text file from dataframe for spm input \n",
    "            tmp = tempfile.NamedTemporaryFile(delete=False) \n",
    "            tmp.close()\n",
    "            with open(tmp.name, 'w') as tmpout:\n",
    "                for key, value in news_df_title.iteritems():\n",
    "                    tmpout.write(value + '\\n')\n",
    "\n",
    "            with open(tmp.name, 'r') as tmpin:\n",
    "                if not delete_files:\n",
    "                    prefix = \"spm/\" # create a dir to store\n",
    "                    handle_dirs(prefix)\n",
    "                    handle_dirs(prefix)\n",
    "                \n",
    "                prefix = prefix + str(vocab_size) + \"_train_news_spm\"\n",
    "                model_name = prefix + \".model\"\n",
    "                vocab_name = prefix + \".vocab\"\n",
    "\n",
    "                character_coverage = 1.0  # to reduce character set \n",
    "                model_type =\"bpe\"     # choose from unigram (default), bpe, char, or word\n",
    "\n",
    "                templates= \"--input={} --pad_id={} --bos_id={} --eos_id={} --unk_id={} \\\n",
    "                            --model_prefix={} --vocab_size={} \\\n",
    "                            --character_coverage={} --model_type={}\"\n",
    "\n",
    "                cmd = templates.format(tmp.name,\n",
    "                                title_vocab.pad_index,\n",
    "                                title_vocab.bos_index,\n",
    "                                title_vocab.eos_index,\n",
    "                                title_vocab.unk_index,\n",
    "                                prefix, vocab_size,\n",
    "                                character_coverage, model_type)\n",
    "                \n",
    "                spm.SentencePieceTrainer.Train(cmd) # run the trainer on trainset\n",
    "                \n",
    "            os.remove(tmp.name) # delete temp txt file after training\n",
    "            \n",
    "            title_vocab.load_model_file(model_name) # load the model into spm\n",
    "            title_vocab.load_vocab_file(vocab_name) # load the vocab file for saving\n",
    "\n",
    "            if delete_files:\n",
    "                os.remove(model_name)\n",
    "                os.remove(vocab_name)\n",
    "     \n",
    "        return cls(title_vocab, category_vocab, mode) # title_char_vocab, \n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents, mode):\n",
    "        if mode == \"word\":\n",
    "            title_vocab = \\\n",
    "               WordSequenceVocabulary.from_serializable(contents['title_vocab'])\n",
    "        elif mode == \"char\":\n",
    "            title_vocab = \\\n",
    "              CharacterSequenceVocabulary.from_serializable(contents['title_vocab'])\n",
    "        elif \"bpe\" in mode:\n",
    "            title_vocab = \\\n",
    "              SubwordSequenceVocabulary.from_serializable(contents['title_vocab'])      \n",
    "        elif mode == \"sent\":\n",
    "            title_vocab = \\\n",
    "              SentenceSequenceVocabulary.from_serializable(contents['title_vocab'])\n",
    "\n",
    "        category_vocab =  \\\n",
    "            Vocabulary.from_serializable(contents['category_vocab'])\n",
    "\n",
    "        return cls(title_vocab=title_vocab, category_vocab=category_vocab) # title_vocab=title_vocab, \n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'title_vocab': self.title_vocab.to_serializable(),\n",
    "                'category_vocab': self.category_vocab.to_serializable()} \n",
    "    # 'title_vocab': self.title_vocab.to_serializable(),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlDY-mjpC0PE"
   },
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2hvfxY2pC0PF"
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, news_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            news_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (NewsVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.news_df = news_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, news_df.title)) + 2\n",
    "        \n",
    "        self._max_word_length = 0\n",
    "        self._max_sent_length = 0\n",
    "        for title in news_df.title:\n",
    "            if len(title) > self._max_sent_length:\n",
    "                self._max_sent_length = len(title)\n",
    "            for token in title.split(\" \"):\n",
    "                if len(token) > self._max_word_length:\n",
    "                    self._max_word_length = len(token)\n",
    "                    \n",
    "        self.train_df = self.news_df[self.news_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.news_df[self.news_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.news_df[self.news_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        class_counts = news_df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, news_csv, mode, vocab_size):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        train_news_df = news_df[news_df.split=='train']\n",
    "        return cls(news_df, NewsVectorizer.from_dataframe(train_news_df, mode = mode,\n",
    "                                                          vocab_size = vocab_size))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer. \n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(news_csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of SurnameVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        title_vector = \\\n",
    "            self._vectorizer.vectorize(row.title, self._max_seq_length, \n",
    "                                       self._max_word_length, self._max_sent_length)\n",
    "\n",
    "        category_index = \\\n",
    "            self._vectorizer.category_vocab.lookup_token(row.category)\n",
    "\n",
    "        return {'x_data': title_vector,\n",
    "                'y_target': category_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqyWjAcPC0PI"
   },
   "source": [
    "## The Model: NewsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIpn92p9C0PI"
   },
   "outputs": [],
   "source": [
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self, model_mode, char_embedding_size, word_embedding_size, \n",
    "                 char_num_embeddings, word_num_channels, \n",
    "                 char_kernel_size, hidden_dim, num_classes, dropout_p, \n",
    "                 char_pretrained_embeddings=None, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size (int): size of the embedding vectors\n",
    "            num_embeddings (int): number of embedding vectors\n",
    "            filter_width (int): width of the convolutional kernels\n",
    "            num_channels (int): number of convolutional kernels per layer\n",
    "            hidden_dim (int): the size of the hidden dimension\n",
    "            num_classes (int): the number of classes in classification\n",
    "            dropout_p (float): a dropout parameter \n",
    "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
    "                default is None. If provided, \n",
    "            padding_idx (int): an index representing a null position\n",
    "        \"\"\"\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        print((\"model_mode={}, char_embedding_size={}, word_embedding_size={}, char_num_embeddings={}, word_num_channels={}, \" \\\n",
    "              + \"char_kernel_size={}, hidden_dim={}, num_classes={}\" \\\n",
    "              + \"\").format(model_mode, char_embedding_size, word_embedding_size, char_num_embeddings, word_num_channels, \n",
    "                 char_kernel_size, hidden_dim, num_classes))\n",
    "        self.model_mode = model_mode\n",
    "\n",
    "        if \"word\" in self.model_mode or self.model_mode == \"sent\":\n",
    "            if char_pretrained_embeddings is None: # token_emb \n",
    "                self.char_emb = nn.Embedding(embedding_dim=word_embedding_size,\n",
    "                                        num_embeddings=char_num_embeddings,\n",
    "                                        padding_idx=padding_idx)        \n",
    "            else:\n",
    "                char_pretrained_embeddings = torch.from_numpy(char_pretrained_embeddings).float()\n",
    "                self.char_emb = nn.Embedding(embedding_dim=word_embedding_size,\n",
    "                                        num_embeddings=char_num_embeddings,\n",
    "                                        padding_idx=padding_idx,\n",
    "                                        _weight=char_pretrained_embeddings)\n",
    "        elif \"char\" in self.model_mode: \n",
    "            if char_pretrained_embeddings is None: # char_emb\n",
    "                self.char_emb = nn.Embedding(embedding_dim=char_embedding_size,\n",
    "                                            num_embeddings=char_num_embeddings,\n",
    "                                            padding_idx=padding_idx)        \n",
    "            else:\n",
    "                char_pretrained_embeddings = torch.from_numpy(char_pretrained_embeddings).float()\n",
    "                self.char_emb = nn.Embedding(embedding_dim=char_embedding_size,\n",
    "                                            num_embeddings=char_num_embeddings,\n",
    "                                            padding_idx=padding_idx,\n",
    "                                            _weight=char_pretrained_embeddings)\n",
    "        \n",
    "            self.char_convnet = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=char_embedding_size, out_channels=word_embedding_size, kernel_size=char_kernel_size),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.word_convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=word_embedding_size, \n",
    "                   out_channels=word_num_channels, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=word_num_channels, out_channels=word_num_channels, \n",
    "                   kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=word_num_channels, out_channels=word_num_channels, \n",
    "                   kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=word_num_channels, out_channels=word_num_channels, \n",
    "                   kernel_size=3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(word_num_channels, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, dataset._max_seq_length)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # embed and permute so features are channels\n",
    "        # x_in: (batch_size, max_seq_size, max_word_size)\n",
    "        # x_emb: (batch_size, max_seq_size, max_word_size, char_embedding_size)\n",
    "        x_emb = self.char_emb(x_in)\n",
    "\n",
    "        if \"char\" in self.model_mode: # char or bpe-char\n",
    "            batch_size = x_emb.size(dim=0)\n",
    "            max_seq_size = x_emb.size(dim=1)\n",
    "            max_word_size = x_emb.size(dim=2)\n",
    "            char_embedding_size = x_emb.size(dim=3)\n",
    "            # x_reshaped: (batch_size * max_seq_size, char_embedding_size, max_word_size)\n",
    "            x_reshaped = x_emb.view(batch_size * max_seq_size, max_word_size, char_embedding_size).permute(0, 2, 1)\n",
    "\n",
    "            # x_conv: (batch_size * max_seq_size, word_embedding_size, max_word_size - char_kernel_size + 1)\n",
    "            x_conv = self.char_convnet(x_reshaped)\n",
    "            # x_conv_out: (batch_size * max_seq_size, word_embedding_size)\n",
    "            word_embedding_size = x_conv.size(dim=1)\n",
    "            remaining_size = x_conv.size(dim=2)\n",
    "            x_conv_out = F.max_pool1d(x_conv, remaining_size).squeeze(dim=2)\n",
    "            x_embedding = x_conv_out.view(batch_size, max_seq_size, word_embedding_size)\n",
    "        elif \"word\" in self.model_mode or self.model_mode == \"sent\":\n",
    "            x_embedding = x_emb\n",
    "        \n",
    "        features = self.word_convnet(x_embedding.permute(0, 2, 1))\n",
    "\n",
    "        # average and remove the extra dimension\n",
    "        remaining_size = features.size(dim=2)\n",
    "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
    "        features = F.dropout(features, p=self._dropout_p)\n",
    "        \n",
    "        # mlp classifier\n",
    "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jRbYNNxoC0PK"
   },
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRWg3BxEC0PL"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cB77O3A1C0PL"
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fQfw6SqPC0PN"
   },
   "source": [
    "#### general utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIMa66CbC0PN"
   },
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "def load_glove_from_file(glove_filepath):\n",
    "    \"\"\"\n",
    "    Load the GloVe embeddings \n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): path to the glove embeddings file \n",
    "    Returns:\n",
    "        word_to_index (dict), embeddings (numpy.ndarary)\n",
    "    \"\"\"\n",
    "\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, \"r\", encoding='utf8') as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for a specific set of words.\n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): file path to the glove embeddigns\n",
    "        words (list): list of words in the dataset\n",
    "    \"\"\"\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ug51YzMLC0PR"
   },
   "source": [
    "### Settings and some prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q3g36XC1C0PS"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16543,
     "status": "ok",
     "timestamp": 1586004194322,
     "user": {
      "displayName": "Banausic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYVsjWa_XDZoiSoyArYulTOek6XUfsvUZixt6_=s64",
      "userId": "07824873713850476084"
     },
     "user_tz": -480
    },
    "id": "tD-fOfG7C0PU",
    "outputId": "0eb663bc-f540-42c9-d846-f0e6d931ecc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch5/document_classification/10000_sent_vectorizer.json\n",
      "\tmodel_storage/ch5/document_classification/10000_sent_model.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    news_csv=\"data/ag_news/news_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/ch5/document_classification\",\n",
    "    model_mode = \"sent\", # choose from word, char, bpe-char, bpe-word, sent\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='data/glove/glove.6B.100d.txt', \n",
    "    use_glove=False,\n",
    "    word_embedding_size=100, \n",
    "    char_embedding_size=50,\n",
    "    char_kernel_size=5,\n",
    "    hidden_dim=100, \n",
    "    word_num_channels=100,\n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    weight_decay=1e-5, # Newly added to regularize variance\n",
    "    dropout_p=0.2, #0.1\n",
    "    batch_size= 128,\n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    vocab_size = 10000, # 1000, 3000, 10000\n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ") \n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = args.model_mode + \"_\" + args.vectorizer_file\n",
    "    args.model_state_file = args.model_mode + \"_\" + args.model_state_file\n",
    "    if \"bpe\" in args.model_mode or args.model_mode == \"sent\":\n",
    "        args.vectorizer_file = str(args.vocab_size) + \"_\" + args.vectorizer_file\n",
    "        args.model_state_file = str(args.vocab_size) + \"_\" + args.model_state_file\n",
    "\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "    \n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzA1NHAaC0PX"
   },
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKUf6plpC0PX"
   },
   "outputs": [],
   "source": [
    "args.use_glove = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15875,
     "status": "ok",
     "timestamp": 1586004215368,
     "user": {
      "displayName": "Banausic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYVsjWa_XDZoiSoyArYulTOek6XUfsvUZixt6_=s64",
      "userId": "07824873713850476084"
     },
     "user_tz": -480
    },
    "id": "XB1QULtbC0PZ",
    "outputId": "4591035e-62b4-4556-baf6-71fe33c6474e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title vocabulary size: 10000\n",
      "Max sentence length: 115\n",
      "Max sequence length: 21\n",
      "Max word length: 35\n",
      "Not using pre-trained embeddings\n",
      "model_mode=sent, char_embedding_size=50, word_embedding_size=100, char_num_embeddings=10000, word_num_channels=100, char_kernel_size=5, hidden_dim=100, num_classes=4\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = NewsDataset.load_dataset_and_load_vectorizer(args.news_csv,\n",
    "                                                           args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = NewsDataset.load_dataset_and_make_vectorizer(args.news_csv, \n",
    "                                                           mode = args.model_mode, \n",
    "                                                           vocab_size = args.vocab_size)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "print(\"Title vocabulary size:\", len(vectorizer.title_vocab))\n",
    "print(\"Max sentence length:\", dataset._max_sent_length)\n",
    "print(\"Max sequence length:\", dataset._max_seq_length)\n",
    "print(\"Max word length:\", dataset._max_word_length)\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.title_vocab._token_to_idx.keys()\n",
    "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n",
    "                                       words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = NewsClassifier(model_mode = args.model_mode,\n",
    "                            char_embedding_size=args.char_embedding_size, \n",
    "                            word_embedding_size=args.word_embedding_size,\n",
    "                            char_num_embeddings=len(vectorizer.title_vocab),\n",
    "                            word_num_channels=args.word_num_channels,\n",
    "                            char_kernel_size=args.char_kernel_size,\n",
    "                            hidden_dim=args.hidden_dim, \n",
    "                            num_classes=len(vectorizer.category_vocab), \n",
    "                            dropout_p=args.dropout_p,\n",
    "                            char_pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)\n",
    "# print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xjxGxz8NC0Pb"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "b9b262f1d38e4474a4dc34187f252ddb",
      "d9fdaab286bf46ea9a29a0953e764cf9",
      "9d664f31a1874432b7e98a8e30a3dd85",
      "09d3e392c9564e149d6ebe54201951cf",
      "485369e8103a4dd0a6c047c8f1f517d0",
      "7609d53983cf4c64ae6e5f6c2bbe9a66",
      "8528d44aa270454b8a2aeb9125ea1e04",
      "279756cb759f4efea133f9aa6e765d49",
      "0105cb1b30e3495bb867816c63019e3c",
      "6c181b31f8024c6d96eff911da12675b",
      "f4816b54535347778d0cca8a4182afe9",
      "fbf73f64b79f4dd1b8cf3d48997fa2bd",
      "223c7d52e54a45e2a01a39aeaea674a1",
      "3480aff494b44c2f9f8ca2e87efbcd6d",
      "df87c06505104ce595d39474dc2a2b97",
      "85601091aeb3405ab63ba45b00a6defb",
      "52d23709e0124978a104d3ac5db443cd",
      "3838672add9e4f8e9aa0f7544eb31f62",
      "2990caa532ce4e3eb33edac5eeb86159",
      "bda1baac71274e46bb3879e31e4b0a82",
      "3ecad7d44eab45e5a1a2db4e740c17f0",
      "65419a3ebaad40698d5391ee780cbfcb",
      "e862a0cafb334453bd8ad74183e417ce",
      "64ae2237214f47a9aed34659ca9018a0"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4344580,
     "status": "ok",
     "timestamp": 1586008550171,
     "user": {
      "displayName": "Banausic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYVsjWa_XDZoiSoyArYulTOek6XUfsvUZixt6_=s64",
      "userId": "07824873713850476084"
     },
     "user_tz": -480
    },
    "id": "zEdWKhqUC0Pb",
    "outputId": "ace92d5d-e86a-4624-c373-eebfdbbfda55"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829d61e500e6449485f4c7bed527d418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13c45db726345898e0ed13583bfe13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=train', max=656.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffa77744539446bae4cd16f1970f3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=val', max=140.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     7: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch     9: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    11: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    13: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    15: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch    17: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch    19: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch    21: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch    23: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch    25: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch    27: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch    29: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch    33: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch    35: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.5259e-08.\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1, verbose=True) #turned on verbose\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                      total=args.num_epochs,\n",
    "                      position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                      total=dataset.get_num_batches(args.batch_size), \n",
    "                      position=1, \n",
    "                      leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                    total=dataset.get_num_batches(args.batch_size), \n",
    "                    position=1, \n",
    "                    leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPB3qYEmC0Pd"
   },
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "        \n",
    "classifier = classifier.to(args.device)   \n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test') \n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4247982,
     "status": "ok",
     "timestamp": 1586008555713,
     "user": {
      "displayName": "Banausic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYVsjWa_XDZoiSoyArYulTOek6XUfsvUZixt6_=s64",
      "userId": "07824873713850476084"
     },
     "user_tz": -480
    },
    "id": "UAkr6GA6C0Pg",
    "outputId": "5bfb49a5-8c91-4f43-830f-34beadfca1ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8477242233497758;\n",
      "Test Accuracy: 82.52232142857146\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gHlT5Gr5C0Ph"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUO6RBjkC0Pi"
   },
   "outputs": [],
   "source": [
    "# Preprocess the reviews\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJY1_yCNC0Pj"
   },
   "outputs": [],
   "source": [
    "def predict_category(title, classifier, vectorizer, max_seq_length, max_word_length, max_sent_length):\n",
    "    \"\"\"Predict a News category for a new title\n",
    "    \n",
    "    Args:\n",
    "        title (str): a raw title string\n",
    "        classifier (NewsClassifier): an instance of the trained classifier\n",
    "        vectorizer (NewsVectorizer): the corresponding vectorizer\n",
    "        max_length (int): the max sequence length\n",
    "            Note: CNNs are sensitive to the input data tensor size. \n",
    "                  This ensures to keep it the same size as the training data\n",
    "    \"\"\"\n",
    "    title = preprocess_text(title)\n",
    "    vectorized_title = \\\n",
    "        torch.tensor(vectorizer.vectorize(title, max_seq_length, max_word_length, max_sent_length))\n",
    "    result = classifier(vectorized_title.unsqueeze(0), apply_softmax=True)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    predicted_category = vectorizer.category_vocab.lookup_index(indices.item())\n",
    "\n",
    "    return {'category': predicted_category, \n",
    "            'probability': probability_values.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHCFNxAJC0Pl"
   },
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    samples = {}\n",
    "    for cat in dataset.val_df.category.unique():\n",
    "        samples[cat] = dataset.val_df.title[dataset.val_df.category==cat].tolist()[:5]\n",
    "    return samples\n",
    "\n",
    "val_samples = get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1358,
     "status": "ok",
     "timestamp": 1586010107609,
     "user": {
      "displayName": "Banausic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjYVsjWa_XDZoiSoyArYulTOek6XUfsvUZixt6_=s64",
      "userId": "07824873713850476084"
     },
     "user_tz": -480
    },
    "id": "7gEhCWe6C0Pn",
    "outputId": "97c6ec98-6f49-49c4-e21c-1d673e47cb0a"
   },
   "outputs": [],
   "source": [
    "#title = input(\"Enter a news title to classify: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "for truth, sample_group in val_samples.items():\n",
    "    print(f\"True Category: {truth}\")\n",
    "    print(\"=\"*30)\n",
    "    for sample in sample_group:\n",
    "        prediction = predict_category(sample, classifier, \n",
    "                                      vectorizer, dataset._max_seq_length + 1, \n",
    "                                      dataset._max_word_length + 1, \n",
    "                                      dataset._max_sent_length + 1)\n",
    "        print(\"Prediction: {} (p={:0.2f})\".format(prediction['category'],\n",
    "                                                  prediction['probability']))\n",
    "        print(\"\\t + Sample: {}\".format(sample))\n",
    "    print(\"-\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXHZhY74C0Po"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AI6127-A3-wordsentencepieceV7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": "5",
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0105cb1b30e3495bb867816c63019e3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4816b54535347778d0cca8a4182afe9",
       "IPY_MODEL_fbf73f64b79f4dd1b8cf3d48997fa2bd"
      ],
      "layout": "IPY_MODEL_6c181b31f8024c6d96eff911da12675b"
     }
    },
    "09d3e392c9564e149d6ebe54201951cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_279756cb759f4efea133f9aa6e765d49",
      "placeholder": "​",
      "style": "IPY_MODEL_8528d44aa270454b8a2aeb9125ea1e04",
      "value": " 100/100 [1:12:05&lt;00:00, 42.90s/it]"
     }
    },
    "223c7d52e54a45e2a01a39aeaea674a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "279756cb759f4efea133f9aa6e765d49": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2990caa532ce4e3eb33edac5eeb86159": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "split=val:  99%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65419a3ebaad40698d5391ee780cbfcb",
      "max": 140,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ecad7d44eab45e5a1a2db4e740c17f0",
      "value": 139
     }
    },
    "3480aff494b44c2f9f8ca2e87efbcd6d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3838672add9e4f8e9aa0f7544eb31f62": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ecad7d44eab45e5a1a2db4e740c17f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "485369e8103a4dd0a6c047c8f1f517d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "52d23709e0124978a104d3ac5db443cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2990caa532ce4e3eb33edac5eeb86159",
       "IPY_MODEL_bda1baac71274e46bb3879e31e4b0a82"
      ],
      "layout": "IPY_MODEL_3838672add9e4f8e9aa0f7544eb31f62"
     }
    },
    "64ae2237214f47a9aed34659ca9018a0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65419a3ebaad40698d5391ee780cbfcb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c181b31f8024c6d96eff911da12675b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7609d53983cf4c64ae6e5f6c2bbe9a66": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8528d44aa270454b8a2aeb9125ea1e04": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85601091aeb3405ab63ba45b00a6defb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d664f31a1874432b7e98a8e30a3dd85": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "training routine: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7609d53983cf4c64ae6e5f6c2bbe9a66",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_485369e8103a4dd0a6c047c8f1f517d0",
      "value": 100
     }
    },
    "b9b262f1d38e4474a4dc34187f252ddb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d664f31a1874432b7e98a8e30a3dd85",
       "IPY_MODEL_09d3e392c9564e149d6ebe54201951cf"
      ],
      "layout": "IPY_MODEL_d9fdaab286bf46ea9a29a0953e764cf9"
     }
    },
    "bda1baac71274e46bb3879e31e4b0a82": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64ae2237214f47a9aed34659ca9018a0",
      "placeholder": "​",
      "style": "IPY_MODEL_e862a0cafb334453bd8ad74183e417ce",
      "value": " 139/140 [1:12:05&lt;00:13, 13.17s/it, acc=78.8, epoch=99, loss=1]"
     }
    },
    "d9fdaab286bf46ea9a29a0953e764cf9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df87c06505104ce595d39474dc2a2b97": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e862a0cafb334453bd8ad74183e417ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4816b54535347778d0cca8a4182afe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "split=train: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3480aff494b44c2f9f8ca2e87efbcd6d",
      "max": 656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_223c7d52e54a45e2a01a39aeaea674a1",
      "value": 655
     }
    },
    "fbf73f64b79f4dd1b8cf3d48997fa2bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85601091aeb3405ab63ba45b00a6defb",
      "placeholder": "​",
      "style": "IPY_MODEL_df87c06505104ce595d39474dc2a2b97",
      "value": " 655/656 [1:11:59&lt;00:13, 13.30s/it, acc=96.3, epoch=99, loss=0.126]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
